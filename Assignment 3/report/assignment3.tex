\documentclass{article}

% Set page margin
% http://kb.mit.edu/confluence/pages/viewpage.action?pageId=3907057
\usepackage[margin=2.5cm]{geometry}

% Plots
\usepackage{pgfplots}

% Math operations
\usepackage{amsmath}

% FloatBarrier
\usepackage{placeins}

% Insert images from file path
\usepackage{graphicx}
\usepackage{wrapfig}

\usepackage{caption}

% Mono spaced font
\usepackage{bera}

% Referencing figures, listings etc. 
\usepackage{hyperref}
\usepackage{capt-of}

% Quotes
\usepackage{csquotes}

% some color definitions
\usepackage{xcolor}
\definecolor{cblue}{RGB}{16,78,139}
\definecolor{cred}{RGB}{139,37,0}
\definecolor{cgreen}{RGB}{0,139,0} 
\definecolor{corange}{RGB}{255,160,77}
\definecolor{clightblue}{RGB}{62,137,190}

% normal box
\newcommand{\sqboxs}{1.2ex}% the square size
\newcommand{\sqboxf}{0.6pt}% the border in \sqboxEmpty
\newcommand{\sqbox}[1]{\textcolor{#1}{\rule{\sqboxs}{\sqboxs}}}

% Code highlighting
\usepackage{color}
\usepackage{listings}
\usepackage{setspace}

% Define colors for Python listings
\definecolor{Code}{rgb}{0,0,0}
\definecolor{Decorators}{rgb}{0.5,0.5,0.5}
\definecolor{Numbers}{rgb}{0.5,0,0}
\definecolor{MatchingBrackets}{rgb}{0.25,0.5,0.5}
\definecolor{Keywords}{rgb}{0,0,1}
\definecolor{self}{rgb}{0,0,0}
\definecolor{Strings}{rgb}{0,0.63,0}
\definecolor{Comments}{rgb}{0,0.63,1}
\definecolor{Backquotes}{rgb}{0,0,0}
\definecolor{Classname}{rgb}{0,0,0}
\definecolor{FunctionName}{rgb}{0,0,0}
\definecolor{Operators}{rgb}{0,0,0}
\definecolor{Background}{rgb}{0.92,0.92,0.92}

\lstdefinelanguage{Python}{
	numbers=left,
	numberstyle=\footnotesize,
	numbersep=1em,
	xleftmargin=2em,
	framextopmargin=2em,
	framexbottommargin=2em,
	showspaces=false,
	showtabs=false,
	showstringspaces=false,
	frame=l,
	tabsize=4,
	% Basic
	basicstyle=\ttfamily\small\setstretch{1},
	backgroundcolor=\color{Background},
	% Comments
	commentstyle=\color{Comments}\slshape,
	% Strings
	stringstyle=\color{Strings},
	morecomment=[s][\color{Strings}]{"""}{"""},
	morecomment=[s][\color{Strings}]{'''}{'''},
	% keywords
	morekeywords={import,from,class,def,for,while,if,is,in,elif,else,not,and,or,print,break,continue,return,True,False,None,access,as,,del,except,exec,finally,global,import,lambda,pass,print,raise,try,assert},
	keywordstyle={\color{Keywords}\bfseries},
	% additional keywords
	morekeywords={[2]@invariant,pylab,numpy,np,scipy},
	keywordstyle={[2]\color{Decorators}\slshape},
	emph={self},
	emphstyle={\color{self}\slshape},
	%
}
\linespread{1.3}


% Define new float types so that I can reference them with captionof from the capt-of package. 
% https://tex.stackexchange.com/questions/6157/floating-an-algorithm
% Could be helpful as well: https://tex.stackexchange.com/questions/115499/image-caption-within-newfloat
\usepackage{float}
\newfloat{MyListing}{t}{lop}
\newfloat{Enumerate}{t}{lop2}


\begin{titlepage}
	\title{Assignment 3 - Reinforcement Learning\\
		{\large Course: \textit{Advanced Machine Learning}}}
	\date{\today}	
\end{titlepage}




\begin{document}
	
	\maketitle
	
	\section{Teammembers and Contributions}
	\label{sec:TeammembersAndContributions}
	
	\begin{itemize}
		\item \makebox[0.4\linewidth]{\textbf{Bernd Menia}, \hfill\textit{01316129}:} Report, Programming, Testing
		\item \makebox[0.4\linewidth]{\textbf{Maximilian Samsinger}, \hfill\textit{01115383}:} Main Programming, Testing, Plots
	\end{itemize}
	
	\noindent The difference in our contributions is because Bernd just only started working with Machine Learning, while Maximilian has already prior knowledge in the subject. Thus we worked together, but the main programming work for this assignment was done by Maximilian. \\
	\\
	In this month's assignment we had to program the Mountain Car task, as initially described by Moore's PhD thesis in 1990 \cite{Moore90efficientmemory-based}. Though as for the actual implementation we used the github project from Senadhi as a basis \cite{MountainCar-v02018}. 
	
	
	\section{Task i}
	\label{sec:Taski}
	Before the algorithm starts the policy approximation some values have to be initialized. Our initial state gets reset by calling $state = env.reset()$. Similar to that the first action gets set to $0$, which corresponds to going left. The features get calculated by calling convert\_to\_features(). Depending on the task the flag $polynomial$ changes the behaviour of the function: \textbf{Task i}, $polynomial = false$) Simply append the initial state with the first action and return it. \textbf{Task ii}, $polynomial = true$) Converts the state to a polynomial of degree max\_degree. However, we cap the degree of action at 1, since we cannot expect higher powers of action to deliver meaningful extra features. Hereby the actions can be $0$, $1$ or $2$, corresponding to \textit{go left}, \textit{do nothing} and \textit{go right} respectively.\\
	\\
	The main part of the code consists of 3 nested for-loops as seen in \autoref{code:loops}. The only thing the outermost loop does is to completely run the code n times. This was just done for convenience so that Task iii can be done without having to manually restart the program. The two inner loops are more interesting: The middle loop represents each episode of the algorithm while the innermost loop represents the steps that are made in each generation. 
	
	\begin{figure}[h]
		\caption{Main part of the program with 3 nested for loops and the calculation of the average rewards}
		\label{code:loops}
		\lstinputlisting[language=Python]{code/loops2.py}
	\end{figure}
	
	\FloatBarrier
	
	\noindent At the start of each episode the middle loop chooses the action that gets used, depending on which action is most suitable (see \autoref{code:chooseAction}). From the second episode onwards the action that gets used is chosen by the algorithm, depending on which action is most suitable. This is done creating a random number and comparing it to a threshold, epsilon, which we set to $0.33$. In other words this means that in $33\%$ of all cases the algorithm performs a random action. \\
	\\
	Initially we set the percentage of random actions to just $2\%$, but we quickly realized, that the mountain car almost never reached the top. Even when it did, it quickly unlearned its behaviour independent of the learning rate.  Once we increased the amount of random actions to $33\%$, and therefore kept the exploration rate very high throughout the simulation, the mountain car got to the top a lot of times.

	\begin{figure}
		\caption{Choose an action with which the algorithm proceeds}
		\label{code:chooseAction}
		\lstinputlisting[language=Python]{code/chooseAction.py}
	\end{figure}
	\newpage
	
	\noindent After an action has been chosen the algorithm performs a step. The step also calculates the new state and the respective reward of the previous state in combination with the action. The reward at each step is $-1$ unless the car reaches the goal. In that case the reward is $0.5$ and the simulation ends. \\
	\\
	We update the weights using temporal difference learning. Since we use approximate the action-value function by a linear function with respect to the weights, the gradient is simply the feature vector. We denote the learning rate as learning\_rate and the discounting factor of the return as discount. 
	The exact formula how the weight update gets calculated can be seen in \autoref{code:updateWeights}.
	
	\begin{figure}[h]
		\caption{Function that updates the weights after a step}
		\label{code:updateWeights}
		\lstinputlisting[language=Python]{code/updateWeights.py}
	\end{figure}
	
	\noindent The results from the first Task are visualized in \autoref{plot:averageRewardTask1}. As we can see, the average reward per episode is highly unstable. The rewards appear to be randomly distributed within a a range of $-0.82$ and $-0.68$. Furthermore the average rewards don't seem to converge over multiple episodes. We assume this happens because of the high exploration rate of our algorithm.
	
	\begin{figure}[h]
		\caption{The average reward per episode after 50 runs (Task1)}
		\label{plot:averageRewardTask1}
		
		\begin{tikzpicture}% coordinates
		\begin{axis} [
			width = \linewidth,
			height = 7cm,
			grid=major,
			xlabel = Episode,
			ylabel = Average Reward per Episode,
			legend pos = south east
		]
		
		\addplot[blue, line width = 0.5mm] table [
			x = Episode, 
			y = Average Reward, 
			col sep=comma,
		] {./plots/averageRewardTask1.csv};
		
		\end{axis}
		\end{tikzpicture}
	\end{figure}
	
	
	\section{Task ii \& Task iii}
	\label{sec:TaskiiAndTaskiii}
	Task ii is quite similar to the first one. The difference is that the $convert\_to\_features()$ function get initialized differently. How this is done is already described in \autoref{sec:Taski}. Task ii also has a highly unstable average reward, but compared to Task i the rewards seem to jitter even more. 

	\begin{figure}[h]
		\caption{The average reward per episode after 50 runs (Task2)}
		\label{plot:averageRewardTask2}
		
		\begin{tikzpicture}% coordinates
		\begin{axis} [
			width = \linewidth,
			height = 7cm,
			grid=major,
			xlabel = Episode,
			ylabel = Average Reward per Episode,
			legend pos = south east
		]
		
		\addplot[blue, line width = 0.5mm] table [
			x = Episode, 
			y = Average Reward, 
			col sep=comma,
		] {./plots/averageRewardTask2.csv};
		
		%\addlegendentry{Kernel Regularizer Acc.}
		
		\end{axis}
		\end{tikzpicture}
	\end{figure}
	
	
	\vspace{-1cm}
	\section{Discussion}
	\label{sec:Discussion}
	We ran into some problems while working on this assignment. First of all polynomials seem to be insufficient to approximate the action-value function. If we had to redo the exercise and could freely choose the features, we would try out coarse coding or we would approximate the action-value function with a neural network. \\
	\\
	Another thing to mention is that our algorithm is unstable. Even after letting the algorithm run for $1000+$ episodes the results from the episodes vary strongly as seen by \autoref{plot:averageRewardTask1} and \autoref{plot:averageRewardTask2}. Unfortunately we don't know why this is the case, but we have spoke with multiple colleagues of us from the course and their programs have similar behaviour. \\
	\\
	Finally the major problem in our code is that our algorithm is prone to unlearning previously learned solutions. It may happen that it discovers a good way to drive upon the mountain, but the solution gets overwritten by the next episodes and therefore unlearns what brought it to the goal, which might be the reason why the graphs are so jittery. 
	
	\bibliographystyle{alpha}
	\bibliography{bibliography}
	
\end{document}