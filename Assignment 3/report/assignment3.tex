\documentclass{article}

% Set page margin
% http://kb.mit.edu/confluence/pages/viewpage.action?pageId=3907057
\usepackage[margin=2.5cm]{geometry}

% Plots
\usepackage{pgfplots}

% Math operations
\usepackage{amsmath}

% FloatBarrier
\usepackage{placeins}

% Insert images from file path
\usepackage{graphicx}
\usepackage{wrapfig}

\usepackage{caption}

% Mono spaced font
\usepackage{bera}

% Referencing figures, listings etc. 
\usepackage{hyperref}
\usepackage{capt-of}

% Quotes
\usepackage{csquotes}

% some color definitions
\usepackage{xcolor}
\definecolor{cblue}{RGB}{16,78,139}
\definecolor{cred}{RGB}{139,37,0}
\definecolor{cgreen}{RGB}{0,139,0} 
\definecolor{corange}{RGB}{255,160,77}
\definecolor{clightblue}{RGB}{62,137,190}

% normal box
\newcommand{\sqboxs}{1.2ex}% the square size
\newcommand{\sqboxf}{0.6pt}% the border in \sqboxEmpty
\newcommand{\sqbox}[1]{\textcolor{#1}{\rule{\sqboxs}{\sqboxs}}}

% Code highlighting
\usepackage{color}
\usepackage{listings}
\usepackage{setspace}

% Define colors for Python listings
\definecolor{Code}{rgb}{0,0,0}
\definecolor{Decorators}{rgb}{0.5,0.5,0.5}
\definecolor{Numbers}{rgb}{0.5,0,0}
\definecolor{MatchingBrackets}{rgb}{0.25,0.5,0.5}
\definecolor{Keywords}{rgb}{0,0,1}
\definecolor{self}{rgb}{0,0,0}
\definecolor{Strings}{rgb}{0,0.63,0}
\definecolor{Comments}{rgb}{0,0.63,1}
\definecolor{Backquotes}{rgb}{0,0,0}
\definecolor{Classname}{rgb}{0,0,0}
\definecolor{FunctionName}{rgb}{0,0,0}
\definecolor{Operators}{rgb}{0,0,0}
\definecolor{Background}{rgb}{0.92,0.92,0.92}

\lstdefinelanguage{Python}{
	numbers=left,
	numberstyle=\footnotesize,
	numbersep=1em,
	xleftmargin=2em,
	framextopmargin=2em,
	framexbottommargin=2em,
	showspaces=false,
	showtabs=false,
	showstringspaces=false,
	frame=l,
	tabsize=4,
	% Basic
	basicstyle=\ttfamily\small\setstretch{1},
	backgroundcolor=\color{Background},
	% Comments
	commentstyle=\color{Comments}\slshape,
	% Strings
	stringstyle=\color{Strings},
	morecomment=[s][\color{Strings}]{"""}{"""},
	morecomment=[s][\color{Strings}]{'''}{'''},
	% keywords
	morekeywords={import,from,class,def,for,while,if,is,in,elif,else,not,and,or,print,break,continue,return,True,False,None,access,as,,del,except,exec,finally,global,import,lambda,pass,print,raise,try,assert},
	keywordstyle={\color{Keywords}\bfseries},
	% additional keywords
	morekeywords={[2]@invariant,pylab,numpy,np,scipy},
	keywordstyle={[2]\color{Decorators}\slshape},
	emph={self},
	emphstyle={\color{self}\slshape},
	%
}
\linespread{1.3}


% Define new float types so that I can reference them with captionof from the capt-of package. 
% https://tex.stackexchange.com/questions/6157/floating-an-algorithm
% Could be helpful as well: https://tex.stackexchange.com/questions/115499/image-caption-within-newfloat
\usepackage{float}
\newfloat{MyListing}{t}{lop}
\newfloat{Enumerate}{t}{lop2}


\begin{titlepage}
	\title{Assignment 2 - Convolutional Neural Network\\
		{\large Course: \textit{Advanced Machine Learning}}}
	\date{\today}	
\end{titlepage}




\begin{document}
	
	\maketitle
	
	\section{Teammembers and Contributions}
	\label{sec:TeammembersAndContributions}
	
	\begin{itemize}
		\item \makebox[0.4\linewidth]{\textbf{Bernd Menia}, \hfill\textit{01316129}:} Report, Programming, Testing
		\item \makebox[0.4\linewidth]{\textbf{Maximilian Samsinger}, \hfill\textit{01115383}:} Main Programming, Testing, Plots
	\end{itemize}
	
	\noindent The difference in our contributions is because Bernd just only started working with Machine Learning, while Maximilian has already prior knowledge in the subject. Thus we worked together, but the main programming work for this assignment was done by Maximilian. \\
	\\
	In this month's assignment we had to program the Mountain Car task, as initially described by Moore's PhD thesis in 1990 \cite{Moore90efficientmemory-based}. Though as for the actual implementation we used the github project from Senadhi as a basis \cite{MountainCar-v02018}. 
	
	
	\section{Task i}
	\label{sec:Taski}
	Before the algorithm starts the policy approximation some values have to be initialized. Our initial state gets reset by calling $state = env.reset()$. Similar to that the first action gets set to $0$, which corresponds to going left. The features get calculated by calling convert\_to\_features(). Depending on the task the flag $polynomial$ changes the behaviour of the function: \textbf{Task i}, $polynomial = false$) Simply append the initial state with the first action and return it. \textbf{Task ii}, $polynomial = true$) Converts the state to a polynomial of degree max\_degree. However, we cap the degree of action at 1, since we cannot expect higher powers of action to deliver meaningful extra features. Hereby the actions can be $0$, $1$ or $2$, corresponding to \textit{go left}, \textit{do nothing} and \textit{go right} respectively.\\
	\\
	The main part of the code consists of 3 nested for-loops as seen in \autoref{code:loops}. The only thing the outermost loop does is to completely run the code n times. This was just done for convenience so that Task iii can be done without having to manually restart the program. The two inner loops are more interesting: The middle loop represents each episode of the algorithm while the innermost loop represents the steps that are made in each generation. 
	
	\begin{figure}[h]
		\caption{Main part of the program with 3 nested for loops and the calculation of the average rewards}
		\label{code:loops}
		\lstinputlisting[language=Python]{code/loops2.py}
	\end{figure}
	
	\FloatBarrier
	
	\noindent At the start of each episode the middle loop chooses the action that gets used, depending on which action is most suitable (see \autoref{code:chooseAction}). From the second episode onwards the action that gets used is chosen by the algorithm, depending on which action is most suitable. This is done creating a random number and comparing it to a threshold, epsilon, which we set to $0.33$. In other words this means that in $33\%$ of all cases the algorithm performs a random action. \\
	\\
	Initially we set the percentage of random actions to just $2\%$, but we quickly realized, that the mountain car almost never reached the top. Even when it did, it quickly unlearned its behaviour independent of the learning rate.  Once we increased the amount of random actions to $33\%$, and therefore kept the exploration rate very high throughout the simulation, the mountain car got to the top a lot of times.
	
	%This is done to ensure that even when the algorithm is shifting towards a certain policy and is therefore almost fixed in its actions, there is still the possibility that it performs random actions which might lead to better results. Note that the action also gets chosen anew in each step of each episode (innermost loop). \\
	%\\
	%Initially we set the percentage of random actions to just $2\%$ which we thought would be good, but somehow the results were abysmal and the mountain car almost never got to the top. Once we increased the amount random actions to be $33\%$ the mountain car got to the top a lot of times. 
	
	\begin{figure}
		\caption{Choose an action with which the algorithm proceeds}
		\label{code:chooseAction}
		\lstinputlisting[language=Python]{code/chooseAction.py}
	\end{figure}
	\newpage
	
	\noindent After an action has been chosen the algorithm performs a step. The step also calculates the new state and the respective reward of the previous state in combination with the action. The reward at each step is $-1$ unless the car reaches the goal. In that case the reward is $0.5$ and the simulation ends. \\
	\\
	We update the weights using temporal difference learning. Since we use approximate the action-value function by a linear function with respect to the weights, the gradient is simply the feature vector. We denote the learning rate as learning\_rate and the discounting factor of the return as discount. 
	The exact formula how the weight update gets calculated can be seen in \autoref{code:updateWeights}.
	
	
	
	% Return or reward? 
	%The reward gets calculated as follows: For each step that gets performed and after which the mountain car is not at the goal the reward is -1. On the contrary if after a step the goal got reached the reward is positive, though no exact numbers are given. \\
	%\\
	
	%The only thing left to do is to update the weights so that the policy gets approximated to the best possible values. As long as the goal hasn't been reached the weights get updated by taking into account the action\_value functions of the current- and previous states and also two not yet seen variables, learning\_rate and discount. The learning rate is used to decrease the amount of calculations that we have to make. Theoretically in order to get the best possible policy we would have to perform all combinations of sets and actions that exist. However this amount of complexity is not practical and cannot be achieved in a senseful manner. So instead of trying out all combinations of states and actions we choose one as described in choose\_action() and then dampen the value of the chosen action by multiplying the outcome with the learning\_rate. \\
	%\\
	%The other variable is the discount factor. The discount also acts as a kind of dampening factor, but it is more general than the learning\_rate. The exact formula how weight update gets calculated can be seen in \autoref{code:updateWeights}. 
	
	\begin{figure}[h]
		\caption{Function that updates the weights after a step}
		\label{code:updateWeights}
		\lstinputlisting[language=Python]{code/updateWeights.py}
	\end{figure}
	
	%\noindent The results from the firs Task are visualized in \autoref{plot:averageRewardTask1}. Unfortunately there isn't much to say about the graph as it is highly unstable. The rewards appear to be randomly distributed within a a range of $-0.82$ and $-0.68$. Strange enough the rewards don't seem to converge towards a higher number with each episode which was what we expected at first. We don't know why this isn't the case, but it could very well be that a bug slipped into our code that we haven't identified yet. 
	
	\noindent The results from the first Task are visualized in \autoref{plot:averageRewardTask1}. As we can see, the average reward per episode is highly unstable. The rewards appear to be randomly distributed within a a range of $-0.82$ and $-0.68$. Furthermore the average rewards don't seem to converge over multiple episodes. We assume this happens because of the high exploration rate of our algorithm.
	
	\begin{figure}[h]
		\caption{The average reward per episode after 50 runs (Task1)}
		\label{plot:averageRewardTask1}
		
		\begin{tikzpicture}% coordinates
		\begin{axis} [
			width = \linewidth,
			height = 7cm,
			grid=major,
			xlabel = Episode,
			ylabel = Average Reward per Episode,
			legend pos = south east
		]
		
		\addplot[blue, line width = 0.5mm] table [
			x = Episode, 
			y = Average Reward, 
			col sep=comma,
		] {./plots/averageRewardTask1.csv};
		
		%\addlegendentry{Kernel Regularizer Acc.}
		
		\end{axis}
		\end{tikzpicture}
	\end{figure}
	
	
	\section{Task ii \& Task iii}
	\label{sec:Taskii}
	Task ii is quite similar to the first one. The difference is that the $convert\_to\_features()$ function get initialized differently. How this is done is already described in \autoref{sec:Taski}. Task ii also has a highly unstable average reward, but compared to Task i the rewards seem to jitter even more. 

	\begin{figure}[h]
		\caption{The average reward per episode after 50 runs (Task2)}
		\label{plot:averageRewardTask2}
		
		\begin{tikzpicture}% coordinates
		\begin{axis} [
			width = \linewidth,
			height = 7cm,
			grid=major,
			xlabel = Episode,
			ylabel = Average Reward per Episode,
			legend pos = south east
		]
		
		\addplot[blue, line width = 0.5mm] table [
			x = Episode, 
			y = Average Reward, 
			col sep=comma,
		] {./plots/averageRewardTask2.csv};
		
		%\addlegendentry{Kernel Regularizer Acc.}
		
		\end{axis}
		\end{tikzpicture}
	\end{figure}
	
	
	\vspace{-1cm}
	\section{Discussion}
	%As we have seen we ran into some problems while working on this assignment. First of all using polynomials as the initial values to approximate the action-value function is not good because it is highly likely that these values differ vastly from the actual underlying function that we try to approximate. Of course this is partly inevitable because by definition we don't know the underlying function yet and therefore have to approximate it. But there are far better methods to start the approximation, for example by using a Neural Network at first and then transferring the output to the Reinforcement Algorithms as the input, which is also the process that was used to train AlphaGo. \\
	%\\
	We ran into some problems while working on this assignment. First of all polynomials seem to be insufficient to approximate the action-value function. If we had to redo the exercise and could freely choose the features, we would try out coarse coding or we would approximate the action-value function with a neural network. \\
	\\
	Another thing to mention is that our algorithm is unstable. Even after letting the algorithm run for $1000+$ episodes the results from the episodes vary strongly as seen by \autoref{plot:averageRewardTask1} and \autoref{plot:averageRewardTask2}. Unfortunately we don't know why this is the case, but we have spoke with multiple colleagues of us from the course and their programs have similar behaviour. \\
	\\
	Finally the major problem in our code is that our algorithm is prone to unlearning previously learned solutions. It may happen that it discovers a good way to drive upon the mountain, but the solution gets overwritten by the next episodes and therefore unlearns what brought it to the goal, which might be the reason why the graphs are so jittery. 
	
	\bibliographystyle{alpha}
	\bibliography{bibliography}
	
\end{document}